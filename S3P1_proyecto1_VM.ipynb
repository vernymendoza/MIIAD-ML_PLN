{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0F200N-wqO3"
   },
   "source": [
    "![image info](https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w75FGynzwqO3"
   },
   "source": [
    "# Proyecto 1 - Predicción de popularidad en canción\n",
    "\n",
    "En este proyecto podrán poner en práctica sus conocimientos sobre modelos predictivos basados en árboles y ensambles, y sobre la disponibilización de modelos. Para su desarrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 1: Predicción de popularidad en canción\".\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 4. Sin embargo, es importante que avancen en la semana 3 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 4, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/competitions/miad-2025-12-prediccion-popularidad-en-cancion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiB84TdJwqO4"
   },
   "source": [
    "## Datos para la predicción de popularidad en cancion\n",
    "\n",
    "En este proyecto se usará el conjunto de datos de datos de popularidad en canciones, donde cada observación representa una canción y se tienen variables como: duración de la canción, acusticidad y tempo, entre otras. El objetivo es predecir qué tan popular es la canción. Para más detalles puede visitar el siguiente enlace: [datos](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuzQAykBwqO4"
   },
   "source": [
    "## Ejemplo predicción conjunto de test para envío a Kaggle\n",
    "\n",
    "En esta sección encontrarán el formato en el que deben guardar los resultados de la predicción para que puedan subirlos a la competencia en Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1744039049283,
     "user": {
      "displayName": "Ana Maria Quintero",
      "userId": "04369708050989259997"
     },
     "user_tz": 300
    },
    "id": "5WHlajVlwqO4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1744039050240,
     "user": {
      "displayName": "Ana Maria Quintero",
      "userId": "04369708050989259997"
     },
     "user_tz": 300
    },
    "id": "6wjRyxQWwqO5"
   },
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1454,
     "status": "ok",
     "timestamp": 1744039599021,
     "user": {
      "displayName": "Ana Maria Quintero",
      "userId": "04369708050989259997"
     },
     "user_tz": 300
    },
    "id": "POuKslmYwqO5"
   },
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTrain_Spotify.csv')\n",
    "#dataTraining = pd.read_csv('E:/vhmen/Documents/MIAD - ANDES/9_Machine larning y LPN/3. Servicio en la nube/dataTrain_Spotify.csv')\n",
    "\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2025/main/datasets/dataTest_Spotify.csv', index_col=0)\n",
    "#dataTesting = pd.read_csv('E:/vhmen/Documents/MIAD - ANDES/9_Machine larning y LPN/3. Servicio en la nube/dataTest_Spotify.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZm8t9NBwqO5"
   },
   "outputs": [],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbrTQFErwqO5"
   },
   "outputs": [],
   "source": [
    "# Visualización datos de test\n",
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1744044980160,
     "user": {
      "displayName": "Ana Maria Quintero",
      "userId": "04369708050989259997"
     },
     "user_tz": 300
    },
    "id": "5ly6bRwFwqO5"
   },
   "outputs": [],
   "source": [
    "# Predicción del conjunto de test - acá se genera un número aleatorio como ejemplo\n",
    "np.random.seed(42)\n",
    "y_pred = pd.DataFrame(np.random.rand(dataTesting.shape[0]) * 100, index=dataTesting.index, columns=['Popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8sZll9owqO5"
   },
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "y_pred.to_csv('test_submission_file.csv', index_label='ID')\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento de datos\n",
    "\n",
    "Es esta sección se encuntrará la descripción y arreglo de los datos de entrenamiento para su posterior predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la columna Unnamed: 0 presenta el mismo consecutivo que el index.\n",
    "dataTraining.drop('Unnamed: 0',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de datos de entrenamiento presenta 20 columnas con 79800 filas, lo cual representa el 70% del total de registros, dejando un 30% de datos para el conjunto de prueba. Del total de variables 9 son variables con valores continuos con decimales, 5 son valores enteros, 5 son valores de texto y 1 es un valor booleano. La variable objetivo a predecir es 'popularity' la cual se busca predecir la popularidad de la cación a partir de las caracteristicas en la base de datos como  la duración, si contiene letras explicitas, si es bailable, entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataTraining['popularity'], bins=100, color='skyblue', edgecolor='black')\n",
    "mensaje = ['Histograma de la variable popularidad']\n",
    "plt.title(\" \".join(mensaje))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['popularity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable popularity presenta una distribución con dos picos, uno cercano al 22 y otro cercano al 43, esto sin tener en cuenta que un 14% de los datos presentan un valor de popularidad de cero, que representan a más de 11.000 canciones. Si bien el promedio de popularidad de las canciones es de 33 puntos, esta media presenta una desviación estandar de 22 puntos por lo que se presenta una distribució aplanada. El valor del puntaje puede variar de 0 a 100, siendo 100 una canción con alta popularidad y 0 una canción con baja popularidad. Dado que se tiene una variable con un valor numerico de 0 a 100 se recomendaria inicialmente realizar una predicción mediante regresión, sin embargo se van a considerar más modelos. No se considera ajustar los valores de cero de la distribución debido a que solo representan un 14% de los datos los cuales si pueden dar indicios de la nula popularidad de una cación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 25.775 artistas, 114 generos, 37.315 nombres de albunes diferentes, 55.767 nombres de canciones diferentes, se propone incialmente no tener en cuenta estas variables debido a que se necesita realizar un proceso de segmentación de texto par apoder generalizar los artistas, generos, nombres de albunes y nombres de canciones de tal forma que se puede ver la influencia de cada palabra encontrada en la popularidad de la cancion, asi mismo seria computacionalmente pesado el procesar cada ramificación de los más de 25 mil artistas o incluso el nombre de los albunes o nombres de canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = dataTraining.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_num, kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_Mis=df_num.corr()\n",
    "corr_Mis.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al revisar las variables númericas de la base de datos, las variables de tempo y danceability presentan distribuciones en forma de campana, de resto son distribuciones sesegadas o uniformes. Al comparar las variables con la variable a predecir, estas no presentan correlación significativa con la variable popularity, asi mismo, de las correlaciones entre todas las variables númericas solo la relación loudness-energy parece tener una correlación positiva en donde al aumentar la sonoridad aumenta la energia presentando un coeficiente de correlación del 76%. Por otro lado, otra correlación potencial puede ser la de acousticness-energy la cual es negativa en donde a mayor confianza de que la pista sea acustica menor es la energia, esto se representa con un coeficiente de correlación del -73% sin embargo, al revisar el grafico de puntos esta correlación negativa no es muy clara.\n",
    "\n",
    "A partir de lo anterior se puede mencionar de manera general que las variables numericas explicativas del modelo a proponer no presentan correlaciones significativas y que estas tampoco presentan correlaciones con la variable dependiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining['explicit']=np.where(dataTraining['explicit']==True,1,0)\n",
    "dataTesting['explicit']=np.where(dataTesting['explicit']==True,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selección de train y test y variables dependientes e independientes\n",
    "y_total=dataTraining['popularity']\n",
    "x_total=dataTraining[['duration_ms','explicit','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature']]\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x_total, y_total, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain_1=dataTraining['popularity']\n",
    "xTrain_1=dataTraining[['duration_ms','explicit','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature']]\n",
    "xTest_1=dataTesting[['duration_ms','explicit','danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "xTrain_s = s.fit_transform(xTrain)\n",
    "xTrain_1_s = s.fit_transform(xTrain_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest_s = s.transform(xTest)\n",
    "xTest_1_s = s.transform(xTest_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calibración y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Arboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores para calibrar el criterio de parada de máxima profundidad (max_depth)\n",
    "max_depth_range = range(1, 21)\n",
    "\n",
    "# Lista para guardar los valores del RMSE para cada valor de máxima profundidad (max_depth)\n",
    "MSE_scores_DT = []\n",
    "\n",
    "# Loop para obtener el desempeño del modelo de acuerdo con la máxima profundidad\n",
    "for depth in max_depth_range:\n",
    "    # Definición del árbol de decisión usando DecisionTreeClassifier de la libreria sklearn\n",
    "    clf = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores_DT.append(cross_val_score(clf,  xTrain_s, yTrain, cv=5, scoring='neg_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depth_range, MSE_scores_DT)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('nMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sqrt(np.abs(MSE_scores_DT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_DT, max_depth_range),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeRegressor(max_depth=10, random_state=1)\n",
    "clf_dt.fit(xTrain_s,yTrain)\n",
    "\n",
    "yPred_dt=clf_dt.predict(xTest_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dt_adj=mean_squared_error(yTest, yPred_dt, squared=False)\n",
    "#mse_dt_adj=np.sqrt(mean_squared_error(yTest, yPred_dt))\n",
    "mse_dt_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = range(1, 70)\n",
    "MSE_scores_RF_md = []\n",
    "\n",
    "for estimator in depth_range:\n",
    "    rf = RandomForestRegressor(max_depth=estimator, random_state=1, n_jobs=-1)\n",
    "    scores = cross_val_score(rf, xTrain_s, yTrain, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    MSE_scores_RF_md.append(scores.mean())\n",
    "\n",
    "\n",
    "plt.plot(depth_range, MSE_scores_RF_md)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_RF_md, depth_range),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de n_estimators\n",
    "estimator_range = range(5, 150, 5)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de n_estimators\n",
    "MSE_scores_RF = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de n_estimators\n",
    "for estimator in estimator_range:\n",
    "    clf4 = RandomForestRegressor(n_estimators=estimator, random_state=1, n_jobs=-1)\n",
    "    MSE_scores_RF.append(cross_val_score(clf4,  xTrain_s, yTrain, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "\n",
    "plt.plot(estimator_range, MSE_scores_RF)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_RF, estimator_range),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "feature_cols=xTrain.columns\n",
    "feature_range = range(1, len(feature_cols)+1)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "MSE_scores_RF_1 = []\n",
    "\n",
    "# Uso de un 10-fold cross-validation para cada valor de max_features\n",
    "for feature in feature_range:\n",
    "    clf5 = RandomForestRegressor(n_estimators=200, max_features=feature, random_state=1, n_jobs=-1)\n",
    "    MSE_scores_RF_1.append(cross_val_score(clf5, xTrain_s, yTrain, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(feature_range, MSE_scores_RF_1)\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_RF_1, feature_range),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestRegressor(n_estimators=145, max_features=5, max_depth=52, random_state=1, n_jobs=-1)\n",
    "clf_rf.fit(xTrain_s,yTrain)\n",
    "\n",
    "yPred_rf=clf_rf.predict(xTest_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_rf_adj=mean_squared_error(yTest, yPred_rf, squared=False)\n",
    "#mse_rf_adj=np.sqrt(mean_squared_error(yTest, yPred_rf))\n",
    "mse_rf_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_op=[0.01, 0.05, 0.1, 0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7]\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "MSE_scores_XGB = []\n",
    "\n",
    "# Uso de un 10-fold cross-validation para cada valor de max_features\n",
    "for LRate in learning_rate_op:\n",
    "    clf7 = XGBRegressor(learning_rate=LRate)\n",
    "    MSE_scores_XGB.append(cross_val_score(clf7, xTrain_s, yTrain,cv=5, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "# Gráfica del desempeño del modelo vs la cantidad de max_features\n",
    "plt.plot(learning_rate_op, MSE_scores_XGB)\n",
    "plt.xlabel('Learning_rate')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_XGB, learning_rate_op),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_op=[0, 0.1, 0.5, 1]\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "MSE_scores_XGB_g = []\n",
    "\n",
    "# Uso de un 10-fold cross-validation para cada valor de max_features\n",
    "for gam in gamma_op:\n",
    "    clf7 = XGBRegressor(gamma=gam)\n",
    "    MSE_scores_XGB_g.append(cross_val_score(clf7, xTrain_s, yTrain,cv=5, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "\n",
    "# Gráfica del desempeño del modelo vs la cantidad de max_features\n",
    "plt.plot(gamma_op, MSE_scores_XGB_g)\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_XGB_g, gamma_op),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsample_bytree_op=[0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "MSE_scores_XGB_c = []\n",
    "\n",
    "# Uso de un 10-fold cross-validation para cada valor de max_features\n",
    "for csbt in colsample_bytree_op:\n",
    "    clf7 = XGBRegressor(colsample_bytree=csbt)\n",
    "    MSE_scores_XGB_c.append(cross_val_score(clf7, xTrain_s, yTrain,cv=5, scoring='neg_mean_squared_error').mean())\n",
    "\n",
    "\n",
    "# Gráfica del desempeño del modelo vs la cantidad de max_features\n",
    "plt.plot(colsample_bytree_op, MSE_scores_XGB_c)\n",
    "plt.xlabel('Colsample')\n",
    "plt.ylabel('nMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(MSE_scores_XGB_c, colsample_bytree_op),reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_Adj = XGBRegressor(learning_rate=0.35,gamma=1,colsample_bytree=1)\n",
    "clf_xgb_Adj.fit(xTrain_s, yTrain)\n",
    "y_pred_XGB_adj = clf_xgb_Adj.predict(xTest_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_XGb_adj=mean_squared_error(yTest, y_pred_XGB_adj, squared=False)\n",
    "#mse_XGb_adj=np.sqrt(mean_squared_error(yTest, y_pred_XGB_adj))\n",
    "mse_XGb_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_title(\"Comparación de MSE entre los modelos\")\n",
    "\n",
    "ejeX = ['Decision Tree Ajustado','Random Forest Ajustado' ,'XGBoost ajustado']\n",
    "ejeY = [mse_dt_adj,mse_rf_adj,mse_XGb_adj]\n",
    "\n",
    "ax.bar(ejeX, ejeY)\n",
    "\n",
    "def addlabels(x, y, plotP):\n",
    "    for i in range(len(x)):\n",
    "        plotP.text(i, y[i], f\"{y[i]:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "addlabels(ejeX, ejeY, plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con base en el MSE se selecciona el modelo de Random Forest Ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacar predicción para Kaggle\n",
    "clf_rf = RandomForestRegressor(n_estimators=145, max_features=5, max_depth=52, random_state=1, n_jobs=-1)\n",
    "clf_rf.fit(xTrain_1_s,yTrain_1)\n",
    "\n",
    "yPred_rf_Kg=clf_rf.predict(xTest_1_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "y_pred_kg = pd.DataFrame(yPred_rf_Kg, index=dataTesting.index, columns=['Popularity'])\n",
    "y_pred_kg.to_csv('E:/vhmen/Documents/MIAD - ANDES/9_Machine larning y LPN/3. Servicio en la nube/test_submission_file_grupo_29_2.csv', index_label='ID')\n",
    "y_pred_kg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Disponibilización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "from flask import Flask\n",
    "from flask_restx import Api, Resource, fields, reqparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamiento del modelo seleccionado\n",
    "clf_rf = RandomForestRegressor(n_estimators=145, max_features=5, max_depth=52, random_state=1, n_jobs=-1)\n",
    "cross_val_score(clf_rf,xTrain_1,yTrain_1, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf.fit(xTrain_1,yTrain_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf_rf, 'E:/vhmen/Documents/MIAD - ANDES/9_Machine larning y LPN/3. Servicio en la nube/train_music_P2.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_m(duration_ms,explicit,danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,time_signature):\n",
    "\n",
    "    clf = joblib.load('E:/vhmen/Documents/MIAD - ANDES/9_Machine larning y LPN/3. Servicio en la nube/train_music_P2.pkl')\n",
    "    \n",
    "    datatrack={'duration_ms':[duration_ms],\n",
    "          'explicit':[explicit],\n",
    "          'danceability':[danceability],\n",
    "          'energy':[energy],\n",
    "          'key':[key],\n",
    "          'loudness':[loudness],\n",
    "          'mode':[mode],\n",
    "          'speechiness':[speechiness],\n",
    "          'acousticness':[acousticness],\n",
    "          'instrumentalness':[instrumentalness],\n",
    "          'liveness':[liveness],\n",
    "          'valence':[valence],\n",
    "          'tempo':[tempo],\n",
    "          'time_signature':[time_signature]}\n",
    "    datatrack = pd.DataFrame(datatrack)\n",
    "\n",
    "    # Make prediction\n",
    "    p1 = clf.predict(datatrack)\n",
    "\n",
    "    return p1\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Definición API Flask\n",
    "api = Api(\n",
    "    app, \n",
    "    version='1.0', \n",
    "    title='Popularity track Prediction API',\n",
    "    description='Popularity track prediction API')\n",
    "\n",
    "ns = api.namespace('predict', \n",
    "     description='Regression to find the Popularity of a Song')\n",
    "\n",
    "# Definición argumentos o parámetros de la API\n",
    "parser = reqparse.RequestParser()\n",
    "parser.add_argument(\n",
    "    'duration_ms', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='Duration of a song in ms', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'explicit', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='Is 1 if the song have a explicit lirycs', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'danceability', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='A value of 0.0 is less danceable and 1.0 is totally danceable.', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'energy', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='A value between 0.0 and 1.0 that represent the intensity and perceptual activity of the track', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'key', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='The key in which the track is located', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'loudness', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='The overall loudness of the track in decibels', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'mode', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='Indicates the mode (major or minor) of the track, 1: major, 0:minor', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'speechiness', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='Detects the presence of spoken words in the track. 1 indicate that the track is entirely spoken and 0.33 indicate a mix between music and speak.', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'acousticness', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1 is a high confidence that the track is acoustic. ', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'instrumentalness', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='Predicts if a track contains no vocals. 1 means that the track have a high probability to be instrumental', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'liveness', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='Detects the presence of an audience in the recording. 1 means that the track is live record.', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'valence', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='A measure from 0 to 1 that describes the musical positivity of the track, high values mean a positive track', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'tempo', \n",
    "    type=float, \n",
    "    required=True, \n",
    "    help='The estimated tempo of the track in beats per minute (BPM)', \n",
    "    location='args')\n",
    "parser.add_argument(\n",
    "    'time_signature', \n",
    "    type=int, \n",
    "    required=True, \n",
    "    help='An estimated time signature, indicating how many beats there are in each measure. Values between 3 to 7 that means 3/4, 4/4...', \n",
    "    location='args')\n",
    "\n",
    "resource_fields = api.model('Resource', {\n",
    "    'result': fields.String,\n",
    "})\n",
    "\n",
    "\n",
    "@ns.route('/')\n",
    "class PopularityTrackApi(Resource):\n",
    "    @ns.expect(parser)\n",
    "    #@ns.doc(parser=parser)\n",
    "    @ns.marshal_with(resource_fields)\n",
    "    def get(self):\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        try: \n",
    "            prediction = predict_m(args['duration_ms'],\n",
    "            args['explicit'],\n",
    "            args['danceability'],\n",
    "            args['energy'],\n",
    "            args['key'],\n",
    "            args['loudness'],\n",
    "            args['mode'],\n",
    "            args['speechiness'],\n",
    "            args['acousticness'],\n",
    "            args['instrumentalness'],\n",
    "            args['liveness'],\n",
    "            args['valence'],\n",
    "            args['tempo'],\n",
    "            args['time_signature'])\n",
    "            return {\"result\":prediction}, 200\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}, 400\n",
    "\n",
    "\n",
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "289160,False,0.409,0.197,0,-13.803,1,0.0294\t,0.797000,0.000319,0.2670,0.0615,91.952,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:5000/predict/?duration_ms=289160&explicit=0&danceability=0.409&energy=0.197&key=0&loudness=-13.803&mode=1&speechiness=0.0294&acousticness=0.797000&instrumentalness=0.000319&liveness=0.2670&valence=0.0615&tempo=91.952&time_signature=4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
